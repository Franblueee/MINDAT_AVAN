{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_learning.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1P-TwIFqhi7gQZsQKo802XxRWVqwrGtJW","authorship_tag":"ABX9TyMpS9zU7+bn+nenPPIKEtYP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8ADaD9J4Qe8J"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","import os\n","path_wd='./drive/MyDrive/MineriaIII'\n","os.chdir(path_wd)"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","import re\n","import tensorflow as tf\n","import keras\n","import pandas as pd\n","import cv2\n","\n","path_models='models/'\n","path_pred='predictions/'\n","path_data='Sentinel2LULC_354/'\n","path_test='Test 2/'\n","ext_sub = '.csv'"],"metadata":{"id":"r3G2EevyKdDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import save_model, load_model"],"metadata":{"id":"x5W85My6P2cC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = []\n","labels = []\n","labelnames = {}\n","\n","for d in os.listdir(path_data):\n","    lab = int(re.search('\\d{1,2}', d).group())\n","    labelnames[str(lab)] = re.search('_+([a-zA-Z]+)_+', d).group(1)\n","    for f in os.listdir(path_data + d):\n","        try:\n","            img = cv2.imread(path_data + d + '/' + f)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            data.append(img)\n","            labels.append(lab)\n","        except:\n","            print(\"Cannot read file: \" + d + '/' + f)\n","\n","data = np.array(data)\n","labels = np.array(labels)"],"metadata":{"id":"OZzSwQbjQjF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, val = train_test_split(np.arange(len(labels)), test_size=0.1, random_state=5, stratify=labels)\n","train_data = data[train]\n","train_labels = labels[train]\n","val_data = data[val]\n","val_labels = labels[val]"],"metadata":{"id":"5Lrb9c2GnQvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data = []\n","test_labels = []\n","test_ids = []\n","\n","for f in os.listdir(path_test):\n","    lab = int(re.search('^(\\d{1,2})_', f).group(1))\n","    try:\n","        img = cv2.imread(path_test + f)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        test_data.append(img)\n","        test_labels.append(lab)\n","        test_ids.append(f)\n","    except:\n","        print(\"Cannot read file: \" + f)\n","        \n","test_data = np.array(test_data)\n","test_labels = np.array(test_labels)"],"metadata":{"id":"HUgdYcIAQlL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","shuffle_buffer_size = 1000\n","\n","nclasses = len(np.unique(labels))\n","img_shape = data.shape[1:4]\n","\n","train_labels_coded = tf.one_hot(train_labels-1, depth=nclasses, on_value=1, off_value=0)\n","val_labels_coded = tf.one_hot(val_labels-1, depth=nclasses, on_value=1, off_value=0)\n","test_labels_coded = tf.one_hot(test_labels-1, depth=nclasses, on_value=1, off_value=0)\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(train_data), tf.convert_to_tensor(train_labels_coded)))\n","val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(val_data), tf.convert_to_tensor(val_labels_coded)))\n","test_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(test_data), tf.convert_to_tensor(test_labels_coded)))\n","\n","train_batches = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size)\n","val_batches = val_dataset.batch(batch_size)\n","test_batches = test_dataset.batch(batch_size)"],"metadata":{"id":"W1T22szxQoZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_model(base_model):\n","  base_model.trainable = False\n","  model = tf.keras.Sequential([\n","    base_model,\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    keras.layers.Dense(nclasses)\n","  ])\n","  return model\n","\n","\n","def train_evaluate_model(model, name, lr=0.0001):\n","  earlystopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_categorical_accuracy',\n","    verbose=1,\n","    mode='max',\n","    patience=5,\n","    restore_best_weights=True\n","  )\n","  epochs = 50\n","  #optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=lr, amsgrad=True)\n","  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","  metrics = [tf.keras.metrics.categorical_accuracy]\n","\n","  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","  history = model.fit(train_batches,\n","                      epochs=epochs, \n","                      validation_data=val_batches,\n","                      callbacks=[earlystopping])\n","  save_model(model, path_models + name + '.h5')\n","\n","  acc = history.history['categorical_accuracy']\n","  val_acc = history.history['val_categorical_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  plt.figure(figsize=(8, 8))\n","  plt.subplot(2, 1, 1)\n","  plt.plot(acc, label='Training Accuracy')\n","  plt.plot(val_acc, label='Validation Accuracy')\n","  plt.legend(loc='lower right')\n","  plt.ylabel('Accuracy')\n","  plt.ylim([min(plt.ylim()),1])\n","  plt.title('Training and Validation Accuracy')\n","\n","  plt.subplot(2, 1, 2)\n","  plt.plot(loss, label='Training Loss')\n","  plt.plot(val_loss, label='Validation Loss')\n","  plt.legend(loc='upper right')\n","  plt.ylabel('Cross Entropy')\n","  plt.ylim([0,(np.max(val_loss)+0.01)])\n","  plt.title('Training and Validation Loss')\n","  plt.xlabel('epoch')\n","  plt.show()\n","\n","  test_loss, test_acc = model.evaluate(test_batches)\n","  print('Test loss: ' + str(test_loss) + ', test acc: ' + str(test_acc))\n","  \n","  val_preds = np.argmax(model.predict(val_data), axis=-1)\n","  val_matrix = tf.math.confusion_matrix(val_labels-1, val_preds)\n","  test_preds = np.argmax(model.predict(test_data), axis=-1)\n","  test_matrix = tf.math.confusion_matrix(test_labels-1, test_preds)\n","\n","  fig, ax = plt.subplots(1, 2, figsize=(10,20))\n","  axlabs = [str(l) for l in np.unique(np.sort(train_labels))]\n","\n","  im = ax[0].imshow(val_matrix)\n","  ax[0].set_xticks(np.arange(nclasses))\n","  ax[0].set_yticks(np.arange(nclasses))\n","  ax[0].set_xticklabels(axlabs)\n","  ax[0].set_yticklabels(axlabs)\n","  ax[0].set_title(\"Validation conf matrix\")\n","\n","  im = ax[1].imshow(test_matrix)\n","  ax[1].set_xticks(np.arange(nclasses))\n","  ax[1].set_yticks(np.arange(nclasses))\n","  ax[1].set_xticklabels(axlabs)\n","  ax[1].set_yticklabels(axlabs)\n","  ax[1].set_title(\"Test confusion matrix\")\n","\n","  plt.show()\n","  \n","  \n","def prep_finetuning(model, from_layer):\n","  model.layers[0].trainable = True\n","  for layer in model.layers[0].layers[:from_layer]:\n","    layer.trainable =  False\n","\n","\n","def predictions_file(model, file='predictions'):\n","  predictions = model.predict(test_data)\n","  preds = np.argmax(predictions, axis=-1) + 1\n","  output = pd.DataFrame({'id.jpg': test_ids, 'label':preds})\n","  output.to_csv(path_pred + file + '.csv', header=True, index=False)"],"metadata":{"id":"-YmWbK6MUfda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base = tf.keras.applications.vgg19.VGG19(include_top = False, weights='imagenet',\n","                                         input_shape=img_shape, pooling='avg')\n","model_avgg = create_model(base)\n","train_evaluate_model(model_avgg, 'avgg19_256_128')"],"metadata":{"id":"YoIH17IpRR44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m = load_model(path_models + 'avgg19_256_128.h5')\n","predictions_file(m, 'avgg19_256_128')"],"metadata":{"id":"-2MpsHsHMbSg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m_avgg = load_model(path_models + 'avgg19_256_128.h5')\n","prep_finetuning(m_avgg, 20)\n","train_evaluate_model(m_avgg, 'avgg19_256_128_v2', 0.00001)"],"metadata":{"id":"VMs-Rlo_PNCT"},"execution_count":null,"outputs":[]}]}