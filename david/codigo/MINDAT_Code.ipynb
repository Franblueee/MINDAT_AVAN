{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2y5Xs2cLS_v",
    "outputId": "7d8873d8-0dad-40d6-dfa5-53618903824c"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#import os\n",
    "#path_wd='drive/MyDrive/MINDAT Aspectos Avanzados/lulc-classification'\n",
    "#os.chdir(path_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_ZcuMCdF7uY"
   },
   "source": [
    "Cargamos las bibliotecas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "twRCOTo4xbHP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.utils as np_utils\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from keras.utils import np_utils\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tensorflow.keras import layers  #Para definición de capas\n",
    "from numpy.random import seed           #Para controlar generación de pesos\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #Para procesado de imágenes\n",
    "import tensorflow.random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #Para procesado de imágenes\n",
    "\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.applications.resnet import ResNet101\n",
    "from keras.applications.resnet import ResNet152\n",
    "\n",
    "#from keras.applications import VGG19\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJAQuhwnGSAt"
   },
   "source": [
    "Consideramos unas semillas aleatorias para garantizar reproducibilidad en los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wTLMgEQrxbkD"
   },
   "outputs": [],
   "source": [
    "seed(1)\n",
    "tensorflow.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFMnyMUrGaZk"
   },
   "source": [
    "Fijamos los paths donde se encuentran las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6XeMyPfpHucA"
   },
   "outputs": [],
   "source": [
    "train_dir = \"./Sentinel2LULC_354/Sentinel2LULC_354\"\n",
    "test_dir = \"./Test 2/Test 2\"\n",
    "weights_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzEq0sPTMw6D"
   },
   "source": [
    "Vamos a definir funciones auxiliares que nos ayudarán a visualizar diferentes aspectos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9Pfdi90NVvS"
   },
   "source": [
    "La primera función irá destinada a calcular la precisión total del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F7dwlo4bM0li"
   },
   "outputs": [],
   "source": [
    "def calcularAccuracy(labels, preds):\n",
    "  accuracy = sum(labels == preds)/len(labels)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmu7sDOAS3eb"
   },
   "source": [
    "Vamos a hacer también una función para calcular la precisión por clase en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OANNmz-cTR-0"
   },
   "outputs": [],
   "source": [
    "def calcularAccuracyClases(labels, preds):\n",
    "  well_pred = np.zeros(labels.shape[1]).tolist()\n",
    "  total = np.zeros(labels.shape[1]).tolist()\n",
    "\n",
    "  labels = np.argmax(labels, axis = 1)\n",
    "  preds = np.argmax(preds, axis = 1)\n",
    "\n",
    "  for i in len(labels):\n",
    "    total[labels[i]] += 1\n",
    "    if labels[i] == preds[i]:\n",
    "      well_pred[labels[i]] += 1\n",
    "  \n",
    "  for i in len(well_pred):\n",
    "    well_pred[i] /= float(total[i])\n",
    "  \n",
    "  return well_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdHeGtnUU1AG"
   },
   "source": [
    "Vamos a hacer una función también para visualizar la evolución de la medida de interés (accuracy en este caso) en entrenamiento y validación para cada época"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sYzYPTkUU9IZ"
   },
   "outputs": [],
   "source": [
    "def mostrarEvolucion(hist):\n",
    "  loss = hist.history['loss']\n",
    "  val_loss = hist.history['val_loss']\n",
    "  plt.plot(loss)\n",
    "  plt.plot(val_loss)\n",
    "  plt.legend(['Training loss', 'Validation loss'])\n",
    "  plt.show()\n",
    "\n",
    "  acc = hist.history['accuracy']\n",
    "  val_acc = hist.history['val_accuracy']\n",
    "  plt.plot(acc)\n",
    "  plt.plot(val_acc)\n",
    "  plt.legend(['Training accuracy','Validation accuracy'])\n",
    "  plt.ylim(0,1)           #Meto esto para mejor comparación de gráficas\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AlVCnX4Hux5"
   },
   "source": [
    "Realizamos la lectura de las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qG8QF3BGsb2",
    "outputId": "70d8d6e4-1331-41df-8608-3f2a1d2d05ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LECTURA DE IMÁGENES ---\n",
      "- Cargando imágenes de entrenamiento...\n",
      "- Cargando imágenes de test...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- LECTURA DE IMÁGENES ---\")\n",
    "\n",
    "print(\"- Cargando imágenes de entrenamiento...\")\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for root, _, files in os.walk(train_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):               \n",
    "            x_train.append(cv2.imread(os.path.join(root, file), 1))\n",
    "            y_train.append(int(file.split(\"_\")[0]))\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "names_test = []\n",
    "\n",
    "print(\"- Cargando imágenes de test...\")\n",
    "\n",
    "for root, _, files in os.walk(test_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):               \n",
    "            x_test.append(cv2.imread(os.path.join(root, file), 1))\n",
    "            y_test.append(int(file.split(\"_\")[0]))\n",
    "            names_test.append(file)\n",
    "\n",
    "\n",
    "# Creación de las subclases que se han considerado:\n",
    "# \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "train, val = train_test_split(np.arange(len(y_train)), test_size=0.15, random_state=5, stratify=y_train)\n",
    "\n",
    "x_val = x_train[val]\n",
    "y_val = y_train[val]\n",
    "y_train = y_train[train]\n",
    "x_train = x_train[train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a trabajar ahora en hacer una agrupación de las clases por grupos similares \n",
    "\n",
    "dict_clases = {1:1, 2:2, 3:3, 4:4, 5:4, 6:5, 7:6, 8:7, 9:8, 10:8,\n",
    "               11:8, 12:9, 13:10, 14:11, 15:12, 16:12, 17:13, 18:14, 19:14, 20:14,\n",
    "               21:15, 22:15, 23:16, 24:17, 25:18, 26:18, 27:18, 28:18, 29:19}\n",
    "\n",
    "dict_clases_agua = {1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10,\n",
    "               11:11, 12:12, 13:13, 14:14, 15:15, 16:16, 17:17, 18:18, 19:19, 20:20,\n",
    "               21:21, 22:21, 23:22, 24:23, 25:24, 26:25, 27:26, 28:27, 29:28}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "[15  7 23 ...  8 11 20]\n",
      "29\n",
      "[ 2  8 11 ... 20  4 20]\n",
      "29\n",
      "[10 10 10 ...  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "# Parámetros para la ejecucción\n",
    "subida = \"24\"\n",
    "load_file = 'david/modelos/model' + subida + '.h5'\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "nclasses = 29\n",
    "epochs = 50\n",
    "jerarquico = False\n",
    "jerarquico_agua = False\n",
    "\n",
    "sube_contraste = True\n",
    "\n",
    "\n",
    "if jerarquico:\n",
    "    # Hay por tanto 6 agrupaciones.\n",
    "    y_train_general = np.array([dict_clases[key] for key in y_train])\n",
    "    y_val_general = np.array([dict_clases[key] for key in y_val])\n",
    "    y_test_general = np.array([dict_clases[key] for key in y_test])\n",
    "    axlabs = [\n",
    "    \"1_BarrenLands\",\n",
    "    \"2_MossAndLichen\",\n",
    "    \"3_MossAndLichen\",\n",
    "    \"4_Srubland\",\n",
    "    \"5_ForestsOpDeBr\",\n",
    "    \"6_ForestClDeBr\",\n",
    "    \"7_ForestsDeDeBr\",\n",
    "    \"8_Forest1\",\n",
    "    \"9_ForestsOpEvBr\",\n",
    "    \"10_ForestsClEvBr\",\n",
    "    \"11_ForestsDeEvBr\",\n",
    "    \"12_Forest2\",\n",
    "    \"13_ForestsDeEvNe\",\n",
    "    \"14_Wetland\",\n",
    "    \"15_Water\",\n",
    "    \"16_PermanentSnow\",\n",
    "    \"17_CropSeasWater\",\n",
    "    \"18_Crop\",\n",
    "    \"19_UrbanBlUpArea\",\n",
    "\n",
    "  ]\n",
    "else:\n",
    "    y_train_general = y_train\n",
    "    y_val_general = y_val\n",
    "    y_test_general = y_test\n",
    "    axlabs = [\n",
    "    \"1_BarrenLands\",\n",
    "    \"2_MossAndLichen\",\n",
    "    \"3_MossAndLichen\",\n",
    "    \"4_SrublandOpen\",\n",
    "    \"5_SrublandClose\",\n",
    "    \"6_ForestsOpDeBr\",\n",
    "    \"7_ForestClDeBr\",\n",
    "    \"8_ForestsDeDeBr\",\n",
    "    \"9_ForestsOpDeNe\",\n",
    "    \"10_ForestsClDeNe\",\n",
    "    \"11_ForestsDeDeNe\",\n",
    "    \"12_ForestsOpEvBr\",\n",
    "    \"13_ForestsClEvBr\",\n",
    "    \"14_ForestsDeEvBr\",\n",
    "    \"15_ForestsOpEvNe\",\n",
    "    \"16_ForestsClEvNe\",\n",
    "    \"17_ForestsDeEvNe\",\n",
    "    \"18_WetlandMangro\",\n",
    "    \"19_WetlandSwamps\",\n",
    "    \"20_WetlandMarshl\",\n",
    "    \"21_WaterBodyMari\",\n",
    "    \"22_WaterBodyCont\",\n",
    "    \"23_PermanentSnow\",\n",
    "    \"24_CropSeasWater\",\n",
    "    \"25_CropCerealrri\",\n",
    "    \"26_CropCereaRain\",\n",
    "    \"27_CropBroadlrri\",\n",
    "    \"28_CropBroadRain\",\n",
    "    \"29_UrbanBlUpArea\",\n",
    "\n",
    "  ]\n",
    "    \n",
    "if jerarquico_agua:\n",
    "    y_train_general = np.array([dict_clases_agua[key] for key in y_train])\n",
    "    y_val_general = np.array([dict_clases_agua[key] for key in y_val])\n",
    "    y_test_general = np.array([dict_clases_agua[key] for key in y_test])\n",
    "    axlabs = [\n",
    "    \"1_BarrenLands\",\n",
    "    \"2_MossAndLichen\",\n",
    "    \"3_MossAndLichen\",\n",
    "    \"4_SrublandOpen\",\n",
    "    \"5_SrublandClose\",\n",
    "    \"6_ForestsOpDeBr\",\n",
    "    \"7_ForestClDeBr\",\n",
    "    \"8_ForestsDeDeBr\",\n",
    "    \"9_ForestsOpDeNe\",\n",
    "    \"10_ForestsClDeNe\",\n",
    "    \"11_ForestsDeDeNe\",\n",
    "    \"12_ForestsOpEvBr\",\n",
    "    \"13_ForestsClEvBr\",\n",
    "    \"14_ForestsDeEvBr\",\n",
    "    \"15_ForestsOpEvNe\",\n",
    "    \"16_ForestsClEvNe\",\n",
    "    \"17_ForestsDeEvNe\",\n",
    "    \"18_WetlandMangro\",\n",
    "    \"19_WetlandSwamps\",\n",
    "    \"20_WetlandMarshl\",\n",
    "    \"21_WaterBody\",\n",
    "    \"22_PermanentSnow\",\n",
    "    \"23_CropSeasWater\",\n",
    "    \"24_CropCerealrri\",\n",
    "    \"25_CropCereaRain\",\n",
    "    \"26_CropBroadlrri\",\n",
    "    \"27_CropBroadRain\",\n",
    "    \"28_UrbanBlUpArea\",\n",
    "  ]\n",
    "\n",
    "print(max(y_train_general))\n",
    "print(y_train_general)\n",
    "print(max(y_val))\n",
    "print(y_val)\n",
    "print(max(y_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8726, 224, 224, 3)\n",
      "(8726,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet101 (Functional)      (None, 2048)              42658176  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 29)                59421     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,717,597\n",
      "Trainable params: 59,421\n",
      "Non-trainable params: 42,658,176\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piwi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 [==============================] - 29s 84ms/step - loss: 2.4505 - categorical_accuracy: 0.5573 - val_loss: 1.5448 - val_categorical_accuracy: 0.6766\n",
      "Epoch 2/50\n",
      "273/273 [==============================] - 22s 80ms/step - loss: 1.2825 - categorical_accuracy: 0.7208 - val_loss: 1.5472 - val_categorical_accuracy: 0.7084\n",
      "Epoch 3/50\n",
      "273/273 [==============================] - 22s 79ms/step - loss: 1.0402 - categorical_accuracy: 0.7617 - val_loss: 1.3847 - val_categorical_accuracy: 0.7370\n",
      "Epoch 4/50\n",
      " 84/273 [========>.....................] - ETA: 12s - loss: 0.7715 - categorical_accuracy: 0.8058"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6dbd6e152444>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m \u001b[0mtrain_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_avgg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avgg19_256_128'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \"\"\"\n",
      "\u001b[1;32m<ipython-input-12-6dbd6e152444>\u001b[0m in \u001b[0;36mtrain_evaluate_model\u001b[1;34m(model, name, lr)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   history = model.fit(it,\n\u001b[0m\u001b[0;32m     68\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nclasses = len(np.unique(y_train))\n",
    "img_shape = x_train.shape[1:4]\n",
    "\n",
    "if sube_contraste:\n",
    "    for i in range(x_train.shape[0]):\n",
    "        x_train[i] = tf.image.adjust_contrast(x_train[i], 2)\n",
    "    for i in range(x_test.shape[0]):\n",
    "        x_test[i] = tf.image.adjust_contrast(x_test[i], 2)\n",
    "    for i in range(x_val.shape[0]):\n",
    "        x_val[i] = tf.image.adjust_contrast(x_val[i], 2)\n",
    "\n",
    "y_train_coded = tf.one_hot(y_train_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "y_val_coded = tf.one_hot(y_val_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "y_test_coded = tf.one_hot(y_test_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_train), tf.convert_to_tensor(y_train_coded)))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_val), tf.convert_to_tensor(y_val_coded)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_test), tf.convert_to_tensor(y_test_coded)))\n",
    "\n",
    "train_batches = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "val_batches = val_dataset.batch(batch_size)\n",
    "test_batches = test_dataset.batch(batch_size)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "datagen = ImageDataGenerator(#width_shift_range=0.3,\n",
    "                             #height_shift_range=0.3,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True,\n",
    "                             #rotation_range=180,\n",
    "                             #zoom_range=[0.7, 1/0.7]\n",
    "                             )\n",
    "\n",
    "val_generator = ImageDataGenerator()\n",
    "test_generator = ImageDataGenerator()\n",
    "\n",
    "it = datagen.flow(x_train, y_train_coded, batch_size=32)\n",
    "val_it = val_generator.flow(x_val, y_val_coded)\n",
    "test_it = test_generator.flow(x_test, y_test_coded)\n",
    "\n",
    "def create_model(base_model):\n",
    "  base_model.trainable = False\n",
    "  #for layer in base_model.layers[-80:]:\n",
    "  #  layer.trainable = True\n",
    "  model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Dense(nclasses, activation='softmax')\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "\n",
    "def train_evaluate_model(model, name, lr=0.001):\n",
    "  earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_categorical_accuracy',\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    "  )\n",
    "  #optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr, amsgrad=True)\n",
    "  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "  metrics = [tf.keras.metrics.categorical_accuracy]\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "  history = model.fit(it,\n",
    "                      epochs=epochs, \n",
    "                      validation_data=val_batches,\n",
    "                      callbacks=[earlystopping])\n",
    "  model.save_weights('david/modelos/model' +subida + '.h5')\n",
    "\n",
    "  acc = history.history['categorical_accuracy']\n",
    "  val_acc = history.history['val_categorical_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.plot(acc, label='Training Accuracy')\n",
    "  plt.plot(val_acc, label='Validation Accuracy')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.ylim([min(plt.ylim()),1])\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.plot(loss, label='Training Loss')\n",
    "  plt.plot(val_loss, label='Validation Loss')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.ylabel('Cross Entropy')\n",
    "  plt.ylim([0,(np.max(val_loss)+0.01)])\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.show()\n",
    "\n",
    "  test_loss, test_acc = model.evaluate(test_batches)\n",
    "  print('Test loss: ' + str(test_loss) + ', test acc: ' + str(test_acc))\n",
    "\n",
    "  \n",
    "  val_preds = np.argmax(model.predict(x_val), axis=-1)\n",
    "  val_matrix = confusion_matrix(y_true=y_val_general-1, y_pred=val_preds).astype(np.float)\n",
    "  val_matrix /= val_matrix.astype(np.float).sum(axis=1)\n",
    "\n",
    "\n",
    "  test_preds = np.argmax(model.predict(x_test), axis=-1)\n",
    "  test_probs = model.predict(x_test)\n",
    "  test_matrix = confusion_matrix(y_true=y_test_general-1, y_pred=test_preds).astype(np.float)\n",
    "\n",
    "  \n",
    "  fig, ax = plt.subplots(figsize=(30, 50))         # Sample figsize in inches\n",
    "  sns.heatmap(val_matrix, annot=True, cmap='Blues', cbar=False, linewidths=1, square=True, xticklabels=axlabs, yticklabels=axlabs)\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(30, 50))         # Sample figsize in inches\n",
    "  sns.heatmap(test_matrix, annot=True, cmap='Blues', cbar=False, linewidths=1, square=True, xticklabels=axlabs, yticklabels=axlabs)\n",
    "  plt.show()\n",
    "  \n",
    "  \n",
    "def prep_finetuning(model, from_layer):\n",
    "  model.layers[0].trainable = True\n",
    "  for layer in model.layers[0].layers[:from_layer]:\n",
    "    layer.trainable =  False\n",
    "\n",
    "\n",
    "def predictions_file(model):\n",
    "  predictions = model.predict(x_test)\n",
    "  preds = np.argmax(predictions, axis=-1) + 1\n",
    "  output = pd.DataFrame({'id.jpg': names_test, 'label':preds})\n",
    "  output.to_csv('david/predicciones/pred' + subida + '.csv', header=True, index=False)\n",
    "    \n",
    "    \n",
    "base = tf.keras.applications.ResNet101(include_top=False, weights='imagenet', input_shape=img_shape, pooling='max')\n",
    "model_avgg = create_model(base)\n",
    "\n",
    "\n",
    "print(model_avgg.summary())\n",
    "\n",
    "\n",
    "train_evaluate_model(model_avgg, 'avgg19_256_128')\n",
    "\n",
    "\"\"\"\n",
    "if load_file is not None:\n",
    "    model_avgg.load_weights(load_file)\n",
    "\"\"\"\n",
    "\n",
    "predictions_file(model_avgg)\n",
    "\n",
    "#m_avgg = load_model(path_models + 'avgg19_256_128.h5')\n",
    "#prep_finetuning(m_avgg, 20)\n",
    "#train_evaluate_model(m_avgg, 'avgg19_256_128_v2', 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q26u2ayL74n"
   },
   "source": [
    "Segundo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wONKlvm8Me3V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet101 (Functional)      (None, 2048)              42658176  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 29)                59421     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,717,597\n",
      "Trainable params: 59,421\n",
      "Non-trainable params: 42,658,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsubida = \"6\"\\n\\nload_file = None\\n\\nbatch_size = 32\\nshuffle_buffer_size = 1000\\n\\nnclasses = len(np.unique(y_train))\\nimg_shape = x_train.shape[1:4]\\n\\nx_train2 = x_train.copy()\\nx_val2 = x_val.copy()\\nx_test2 = x_test.copy()\\n\\nfor i in range(x_train2.shape[0]):\\n    x_train2[i] = tf.image.adjust_saturation(x_train2[i], 2)\\n\\nfor i in range(x_val2.shape[0]):\\n    x_val2[i] = tf.image.adjust_saturation(x_val2[i], 2)\\n\\nfor i in range(x_test2.shape[0]):\\n    x_test2[i] = tf.image.adjust_saturation(x_test2[i], 2)\\n\\ny_train_coded = tf.one_hot(y_train-1, depth=nclasses, on_value=1, off_value=0)\\ny_val_coded = tf.one_hot(y_val-1, depth=nclasses, on_value=1, off_value=0)\\ny_test_coded = tf.one_hot(y_test-1, depth=nclasses, on_value=1, off_value=0)\\n\\ntrain_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_train2), tf.convert_to_tensor(y_train_coded)))\\nval_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_val2), tf.convert_to_tensor(y_val_coded)))\\ntest_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_test2), tf.convert_to_tensor(y_test_coded)))\\n\\ntrain_batches = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size)\\nval_batches = val_dataset.batch(batch_size)\\ntest_batches = test_dataset.batch(batch_size)\\n\\n\\nnclasses = 29\\nepochs = 70\\n\\ndatagen = ImageDataGenerator(#width_shift_range=0.3,\\n                             #height_shift_range=0.3,\\n                             #horizontal_flip=True,\\n                             #vertical_flip=True,\\n                             #rotation_range=180,\\n                             #zoom_range=[0.7, 1/0.7]\\n                             )\\n\\nval_generator = ImageDataGenerator()\\ntest_generator = ImageDataGenerator()\\n\\nit = datagen.flow(x_train2, y_train_coded, batch_size=32)\\nval_it = val_generator.flow(x_val2, y_val_coded)\\ntest_it = test_generator.flow(x_test2, y_test_coded)\\n\\n\\n##########################################################################################\\n##########################################################################################\\n##########################################################################################\\n\\n\\nbase = tf.keras.applications.ResNet50(include_top = False, weights=\\'imagenet\\',\\n                                         input_shape=img_shape, pooling=\\'max\\')\\nmodel_avgg = create_model(base)\\n\\n\\nif load_file is not None:\\n    model_avgg.load_weights(load_file)\\ntrain_evaluate_model(model_avgg, \\'avgg19_256_128\\')\\n\\npredictions_file(model_avgg)\\n\\n#m_avgg = load_model(path_models + \\'avgg19_256_128.h5\\')\\n#prep_finetuning(m_avgg, 20)\\n#train_evaluate_model(m_avgg, \\'avgg19_256_128_v2\\', 0.00001)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(model_avgg.summary())\n",
    "\"\"\"\n",
    "subida = \"6\"\n",
    "\n",
    "load_file = None\n",
    "\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "nclasses = len(np.unique(y_train))\n",
    "img_shape = x_train.shape[1:4]\n",
    "\n",
    "x_train2 = x_train.copy()\n",
    "x_val2 = x_val.copy()\n",
    "x_test2 = x_test.copy()\n",
    "\n",
    "for i in range(x_train2.shape[0]):\n",
    "    x_train2[i] = tf.image.adjust_saturation(x_train2[i], 2)\n",
    "\n",
    "for i in range(x_val2.shape[0]):\n",
    "    x_val2[i] = tf.image.adjust_saturation(x_val2[i], 2)\n",
    "\n",
    "for i in range(x_test2.shape[0]):\n",
    "    x_test2[i] = tf.image.adjust_saturation(x_test2[i], 2)\n",
    "\n",
    "y_train_coded = tf.one_hot(y_train-1, depth=nclasses, on_value=1, off_value=0)\n",
    "y_val_coded = tf.one_hot(y_val-1, depth=nclasses, on_value=1, off_value=0)\n",
    "y_test_coded = tf.one_hot(y_test-1, depth=nclasses, on_value=1, off_value=0)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_train2), tf.convert_to_tensor(y_train_coded)))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_val2), tf.convert_to_tensor(y_val_coded)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_test2), tf.convert_to_tensor(y_test_coded)))\n",
    "\n",
    "train_batches = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "val_batches = val_dataset.batch(batch_size)\n",
    "test_batches = test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "nclasses = 29\n",
    "epochs = 70\n",
    "\n",
    "datagen = ImageDataGenerator(#width_shift_range=0.3,\n",
    "                             #height_shift_range=0.3,\n",
    "                             #horizontal_flip=True,\n",
    "                             #vertical_flip=True,\n",
    "                             #rotation_range=180,\n",
    "                             #zoom_range=[0.7, 1/0.7]\n",
    "                             )\n",
    "\n",
    "val_generator = ImageDataGenerator()\n",
    "test_generator = ImageDataGenerator()\n",
    "\n",
    "it = datagen.flow(x_train2, y_train_coded, batch_size=32)\n",
    "val_it = val_generator.flow(x_val2, y_val_coded)\n",
    "test_it = test_generator.flow(x_test2, y_test_coded)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "base = tf.keras.applications.ResNet50(include_top = False, weights='imagenet',\n",
    "                                         input_shape=img_shape, pooling='max')\n",
    "model_avgg = create_model(base)\n",
    "\n",
    "\n",
    "if load_file is not None:\n",
    "    model_avgg.load_weights(load_file)\n",
    "train_evaluate_model(model_avgg, 'avgg19_256_128')\n",
    "\n",
    "predictions_file(model_avgg)\n",
    "\n",
    "#m_avgg = load_model(path_models + 'avgg19_256_128.h5')\n",
    "#prep_finetuning(m_avgg, 20)\n",
    "#train_evaluate_model(m_avgg, 'avgg19_256_128_v2', 0.00001)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKwBKCzbM3QJ"
   },
   "source": [
    "Realizamos la predicción sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZezqaeT6M98y"
   },
   "outputs": [],
   "source": [
    "if jerarquico:\n",
    "    srubland_dict = {4:1, 5:2}\n",
    "    forest_1_dict = {9:1, 10:2, 11:3}\n",
    "    forest_2_dict = {15:1, 16:2}\n",
    "    wetland_dict = {18:1, 19:2, 20:3}\n",
    "    water_dict = {21:1, 22:2}\n",
    "    crop_dict = {25:1, 26:2, 27:3, 28:4}\n",
    "\n",
    "    list_of_dicts = [srubland_dict, forest_1_dict, forest_2_dict, wetland_dict, water_dict, crop_dict]\n",
    "\n",
    "    list_models = []\n",
    "\n",
    "    for dicti in list_of_dicts:\n",
    "        x_train_new = []\n",
    "        x_val_new = []\n",
    "        x_test_new = []\n",
    "        y_train_new = []\n",
    "        y_test_new = []\n",
    "        y_val_new = []\n",
    "\n",
    "        nclasses = len(dicti.keys())\n",
    "\n",
    "        for i in range(x_train.shape[0]):\n",
    "            if y_train[i] in dicti.keys():\n",
    "                x_train_new.append(x_train[i])\n",
    "                y_train_new.append(dicti[y_train[i]])\n",
    "\n",
    "        for i in range(x_val.shape[0]):\n",
    "            if y_val[i] in dicti.keys():\n",
    "                x_val_new.append(x_val[i])\n",
    "                y_val_new.append(dicti[y_val[i]])\n",
    "\n",
    "        for i in range(x_test.shape[0]):\n",
    "            if y_test[i] in dicti.keys():\n",
    "                x_test_new.append(x_test[i])\n",
    "                y_test_new.append(dicti[y_test[i]])\n",
    "\n",
    "        x_train_new = np.array(x_train_new)\n",
    "        x_val_new = np.array(x_val_new)\n",
    "        x_test_new = np.array(x_test_new)\n",
    "        y_train_new = np.array(y_train_new)\n",
    "        y_val_new = np.array(y_val_new)\n",
    "        y_test_new = np.array(y_test_new)\n",
    "\n",
    "\n",
    "\n",
    "        y_train_coded = tf.one_hot(y_train_new-1, depth=nclasses, on_value=1, off_value=0)\n",
    "        y_val_coded = tf.one_hot(y_val_new-1, depth=nclasses, on_value=1, off_value=0)\n",
    "        y_test_coded = tf.one_hot(y_test_new-1, depth=nclasses, on_value=1, off_value=0)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_train_new), tf.convert_to_tensor(y_train_coded)))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_val_new), tf.convert_to_tensor(y_val_coded)))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_test_new), tf.convert_to_tensor(y_test_coded)))\n",
    "\n",
    "        train_batches = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "        val_batches = val_dataset.batch(batch_size)\n",
    "        test_batches = test_dataset.batch(batch_size)\n",
    "\n",
    "        print(x_train.shape)\n",
    "        print(y_train.shape)\n",
    "\n",
    "        epochs = 70\n",
    "\n",
    "        datagen = ImageDataGenerator(#width_shift_range=0.3,\n",
    "                                     #height_shift_range=0.3,\n",
    "                                     #horizontal_flip=True,\n",
    "                                     #vertical_flip=True,\n",
    "                                     #rotation_range=180,\n",
    "                                     #zoom_range=[0.7, 1/0.7]\n",
    "                                     )\n",
    "\n",
    "        val_generator = ImageDataGenerator()\n",
    "        test_generator = ImageDataGenerator()\n",
    "\n",
    "        it = datagen.flow(x_train_new, y_train_coded, batch_size=32)\n",
    "        val_it = val_generator.flow(x_val_new, y_val_coded)\n",
    "        test_it = test_generator.flow(x_test_new, y_test_coded)\n",
    "\n",
    "\n",
    "        print(model_avgg.summary())\n",
    "        new_model_avgg = tf.keras.models.clone_model(model_avgg)\n",
    "\n",
    "        new_model_avgg.pop()\n",
    "\n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(nclasses, activation='softmax')\n",
    "      ])\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        new_model_avgg = tf.keras.Sequential([\n",
    "        new_model_avgg,\n",
    "        tf.keras.layers.Dense(nclasses, activation='softmax')\n",
    "\n",
    "      ])\n",
    "        print(new_model_avgg.summary())\n",
    "\n",
    "        train_evaluate_model(new_model_avgg, 'avgg19_256_128')\n",
    "\n",
    "        list_models.append(new_model_avgg)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKTCaVdrUqkj"
   },
   "source": [
    "Calculamos algunas medidas de interés sobre la predicción realizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gx2D3QlDUvi-"
   },
   "outputs": [],
   "source": [
    "# Final predictions\n",
    "\n",
    "if jerarquico:\n",
    "    srubland_dict = {4:1, 5:2}\n",
    "    forest_1_dict = {9:1, 10:2, 11:3}\n",
    "    forest_2_dict = {15:1, 16:2}\n",
    "    wetland_dict = {18:1, 19:2, 20:3}\n",
    "    water_dict = {21:1, 22:2}\n",
    "    crop_dict = {25:1, 26:2, 27:3, 28:4}\n",
    "\n",
    "    dict_clases = {1:1, 2:2, 3:3, 4:4, 5:4, 6:5, 7:6, 8:7, 9:8, 10:8,\n",
    "                   11:8, 12:9, 13:10, 14:11, 15:12, 16:12, 17:13, 18:14, 19:14, 20:14,\n",
    "                   21:15, 22:15, 23:16, 24:17, 25:18, 26:18, 27:18, 28:18, 29:19}\n",
    "\n",
    "\n",
    "    y_train_coded = tf.one_hot(y_train_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "    y_val_coded = tf.one_hot(y_val_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "    y_test_coded = tf.one_hot(y_test_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_train), tf.convert_to_tensor(y_train_coded)))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_val), tf.convert_to_tensor(y_val_coded)))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_test), tf.convert_to_tensor(y_test_coded)))\n",
    "\n",
    "    train_batches = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "    val_batches = val_dataset.batch(batch_size)\n",
    "    test_batches = test_dataset.batch(batch_size)\n",
    "\n",
    "    print(model_avgg.summary())\n",
    "\n",
    "    test_preds = np.argmax(model_avgg.predict(x_test), axis=-1) + 1\n",
    "\n",
    "    print(np.max(test_preds))\n",
    "\n",
    "    for i in range(test_preds.shape[0]):\n",
    "\n",
    "        key_list=list(dict_clases.keys())\n",
    "        val_list=list(dict_clases.values())\n",
    "        print(test_preds[i])\n",
    "        ind=val_list.index(test_preds[i])\n",
    "        test_preds[i] = key_list[ind]\n",
    "        print(\"---\\n \", test_preds[i])\n",
    "        if test_preds[i] == 4:\n",
    "            new_pred = np.argmax(list_models[0].predict(np.expand_dims(x_test[i], axis=0))) + 1\n",
    "            print(new_pred)\n",
    "            key_list=list(srubland_dict.keys())\n",
    "            print(key_list)\n",
    "            val_list=list(srubland_dict.values())\n",
    "            print(val_list)\n",
    "            ind=val_list.index(new_pred)\n",
    "            print(ind)\n",
    "            test_preds[i] = key_list[ind]\n",
    "\n",
    "        if test_preds[i] == 9:\n",
    "            new_pred = np.argmax(list_models[1].predict(np.expand_dims(x_test[i], axis=0))) + 1\n",
    "            print(new_pred)\n",
    "            key_list=list(forest_1_dict.keys())\n",
    "            print(key_list)\n",
    "            val_list=list(forest_1_dict.values())\n",
    "            print(val_list)\n",
    "            ind=val_list.index(new_pred)\n",
    "            print(ind)\n",
    "            test_preds[i] = key_list[ind]\n",
    "\n",
    "        if test_preds[i] == 15:\n",
    "            new_pred = np.argmax(list_models[2].predict(np.expand_dims(x_test[i], axis=0))) + 1\n",
    "            print(new_pred)\n",
    "            key_list=list(forest_2_dict.keys())\n",
    "            print(key_list)\n",
    "            val_list=list(forest_2_dict.values())\n",
    "            print(val_list)\n",
    "            ind=val_list.index(new_pred)\n",
    "            print(ind)\n",
    "            test_preds[i] = key_list[ind]\n",
    "\n",
    "        if test_preds[i] == 18:\n",
    "            new_pred = np.argmax(list_models[3].predict(np.expand_dims(x_test[i], axis=0))) + 1\n",
    "            print(new_pred)\n",
    "            key_list=list(wetland_dict.keys())\n",
    "            print(key_list)\n",
    "            val_list=list(wetland_dict.values())\n",
    "            print(val_list)\n",
    "            ind=val_list.index(new_pred)\n",
    "            print(ind)\n",
    "            test_preds[i] = key_list[ind]\n",
    "\n",
    "        if test_preds[i] == 21:\n",
    "            new_pred = np.argmax(list_models[4].predict(np.expand_dims(x_test[i], axis=0))) + 1\n",
    "            print(new_pred)\n",
    "            key_list=list(water_dict.keys())\n",
    "            print(key_list)\n",
    "            val_list=list(water_dict.values())\n",
    "            print(val_list)\n",
    "            ind=val_list.index(new_pred)\n",
    "            print(ind)\n",
    "            test_preds[i] = key_list[ind]\n",
    "\n",
    "        if test_preds[i] == 25:\n",
    "            new_pred = np.argmax(list_models[5].predict(np.expand_dims(x_test[i], axis=0))) + 1\n",
    "            print(new_pred)\n",
    "            key_list=list(crop_dict.keys())\n",
    "            print(key_list)\n",
    "            val_list=list(crop_dict.values())\n",
    "            print(val_list)\n",
    "            ind=val_list.index(new_pred)\n",
    "            print(ind)\n",
    "            test_preds[i] = key_list[ind]\n",
    "\n",
    "    print(max(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if jerarquico:\n",
    "    axlabs = [\n",
    "      \"1_BarrenLands\",\n",
    "      \"2_MossAndLichen\",\n",
    "      \"3_MossAndLichen\",\n",
    "      \"4_SrublandOpen\",\n",
    "      \"5_SrublandClose\",\n",
    "      \"6_ForestsOpDeBr\",\n",
    "      \"7_ForestClDeBr\",\n",
    "      \"8_ForestsDeDeBr\",\n",
    "      \"9_ForestsOpDeNe\",\n",
    "      \"10_ForestsClDeNe\",\n",
    "      \"11_ForestsDeDeNe\",\n",
    "      \"12_ForestsOpEvBr\",\n",
    "      \"13_ForestsClEvBr\",\n",
    "      \"14_ForestsDeEvBr\",\n",
    "      \"15_ForestsOpEvNe\",\n",
    "      \"16_ForestsClEvNe\",\n",
    "      \"17_ForestsDeEvNe\",\n",
    "      \"18_WetlandMangro\",\n",
    "      \"19_WetlandSwamps\",\n",
    "      \"20_WetlandMarshl\",\n",
    "      \"21_WaterBodyMari\",\n",
    "      \"22_WaterBodyCont\",\n",
    "      \"23_PermanentSnow\",\n",
    "      \"24_CropSeasWater\",\n",
    "      \"25_CropCerealrri\",\n",
    "      \"26_CropCereaRain\",\n",
    "      \"27_CropBroadlrri\",\n",
    "      \"28_CropBroadRain\",\n",
    "      \"29_UrbanBlUpArea\",\n",
    "\n",
    "      ]\n",
    "\n",
    "    print(test_preds)\n",
    "\n",
    "    test_matrix = confusion_matrix(y_true=y_test-1, y_pred=test_preds-1).astype(np.float)\n",
    "    fig, ax = plt.subplots(figsize=(30, 50))         # Sample figsize in inches\n",
    "    sns.heatmap(test_matrix, annot=True, cmap='Blues', cbar=False, linewidths=1, square=True, xticklabels=axlabs, yticklabels=axlabs)\n",
    "    plt.show()\n",
    "\n",
    "    output = pd.DataFrame({'id.jpg': names_test, 'label':test_preds})\n",
    "    output.to_csv('david/predicciones/pred' + subida + '.csv', header=True, index=False)\n",
    "\n",
    "    print(y_test)\n",
    "    print(test_preds)\n",
    "\n",
    "    print(calcularAccuracy(np.array(y_test), np.array(test_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if jerarquico_agua:\n",
    "    \n",
    "    # hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    water_dict = {21:1, 22:2}\n",
    "    x_train_new = []\n",
    "    x_test_new = []\n",
    "    y_train_new = []\n",
    "    y_test_new = []\n",
    "\n",
    "    nclasses = len(water_dict.keys())\n",
    "\n",
    "    for i in range(x_train.shape[0]):\n",
    "        if y_train[i] in water_dict.keys():\n",
    "            x_train_new.append(x_train[i])\n",
    "            #x_train_new.append(tf.image.adjust_contrast(x_train[i], 0.95))\n",
    "            y_train_new.append(water_dict[y_train[i]])\n",
    "\n",
    "    for i in range(x_val.shape[0]):\n",
    "        if y_val[i] in water_dict.keys():\n",
    "            x_train_new.append(x_val[i])\n",
    "            #x_val_new.append(tf.image.adjust_contrast(x_val[i], 0.95))\n",
    "            y_train_new.append(water_dict[y_val[i]])\n",
    "\n",
    "    for i in range(x_test.shape[0]):\n",
    "        if y_test[i] in water_dict.keys():\n",
    "            x_test_new.append(x_test[i])\n",
    "            #x_test_new.append(tf.image.adjust_contrast(x_test[i], 0.95))\n",
    "            y_test_new.append(water_dict[y_test[i]])\n",
    "            \n",
    "    x_train_new = np.array(x_train_new)\n",
    "    x_test_new = np.array(x_test_new)\n",
    "    y_train_new = np.array(y_train_new)\n",
    "    y_test_new = np.array(y_test_new)\n",
    "    \n",
    "    fl_train_data = x_train_new.reshape(x_train_new.shape[0], -1)\n",
    "    fl_test_data = x_test_new.reshape(x_test_new.shape[0], -1)\n",
    "    \n",
    "    print(fl_train_data.shape)\n",
    "    print(fl_test_data.shape)\n",
    "\n",
    "    pca = PCA(n_components=10)\n",
    "    #pca = KernelPCA(kernel=\"poly\", degree=3)\n",
    "\n",
    "    pca.fit(fl_train_data)\n",
    "    \n",
    "    #sum(pca.eigenvalues_[0:n_components])/sum(pca.eigenvalues_)\n",
    "    print(np.sum(pca.explained_variance_ratio_))\n",
    "    \n",
    "    pca_train = pca.transform(fl_train_data)\n",
    "    pca_test = pca.transform(fl_test_data)\n",
    "    \n",
    "    \n",
    "    print(pca_train.shape)\n",
    "    print(pca_test.shape)\n",
    "    \n",
    "    \n",
    "    #param_dic = {\"C\" : [0.0001, 0.001, 0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 10.0], \"gamma\" : [0.0001, 0.001, 0.1, 1.0, 2.0, 5.0]}\n",
    "\n",
    "    #param_grid = ParameterGrid(param_dic)\n",
    "    #gs = gs.fit(fl_train_data_trans, train_labels)\n",
    "\n",
    "    \n",
    "    svm = SVC(C=10, kernel=\"rbf\")\n",
    "    svm = svm.fit(pca_train, y_train_new)\n",
    "    \n",
    "    train_preds = svm.predict(pca_train)\n",
    "    test_preds = svm.predict(pca_test) \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    print(pca_train.shape)\n",
    "    print(y_train_new.shape)\n",
    "    \n",
    "    rf.fit(pca_train, y_train_new)\n",
    "    \n",
    "    train_preds = rf.predict(pca_train)\n",
    "    test_preds = rf.predict(pca_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(train_preds)\n",
    "    \n",
    "    print(calcularAccuracy(np.array(y_train_new), np.array(train_preds)))\n",
    "    print(\"--------\")\n",
    "\n",
    "    \n",
    "    print(calcularAccuracy(np.array(y_test_new), np.array(test_preds)))\n",
    "    print(\"--------\")\n",
    "    \n",
    "    \"\"\"\n",
    "    y_train_coded = tf.one_hot(y_train_new-1, depth=nclasses, on_value=1, off_value=0)\n",
    "    y_val_coded = tf.one_hot(y_val_new-1, depth=nclasses, on_value=1, off_value=0)\n",
    "    y_test_coded = tf.one_hot(y_test_new-1, depth=nclasses, on_value=1, off_value=0)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_train_new), tf.convert_to_tensor(y_train_coded)))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_val_new), tf.convert_to_tensor(y_val_coded)))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_test_new), tf.convert_to_tensor(y_test_coded)))\n",
    "\n",
    "    train_batches = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "    val_batches = val_dataset.batch(batch_size)\n",
    "    test_batches = test_dataset.batch(batch_size)\n",
    "\n",
    "    epochs = 70\n",
    "\n",
    "    datagen = ImageDataGenerator(#width_shift_range=0.3,\n",
    "                                 #height_shift_range=0.3,\n",
    "                                 #horizontal_flip=True,\n",
    "                                 #vertical_flip=True,\n",
    "                                 #rotation_range=180,\n",
    "                                 #zoom_range=[0.7, 1/0.7]\n",
    "                                 )\n",
    "\n",
    "    val_generator = ImageDataGenerator()\n",
    "    test_generator = ImageDataGenerator()\n",
    "\n",
    "    it = datagen.flow(x_train_new, y_train_coded, batch_size=32)\n",
    "    val_it = val_generator.flow(x_val_new, y_val_coded)\n",
    "    test_it = test_generator.flow(x_test_new, y_test_coded)\n",
    "\n",
    "\n",
    "    print(model_avgg.summary())\n",
    "    new_model_avgg = tf.keras.models.clone_model(model_avgg)\n",
    "    new_model_avgg.pop()\n",
    "\n",
    "    new_model_avgg = tf.keras.Sequential([\n",
    "    new_model_avgg,\n",
    "    tf.keras.layers.Dense(nclasses, activation='softmax')\n",
    "  ])\n",
    "    print(new_model_avgg.summary())\n",
    "    \n",
    "\n",
    "    train_evaluate_model(new_model_avgg, 'avgg19_256_128')\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-18-8acd95cf7614>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-8acd95cf7614>\"\u001b[1;36m, line \u001b[1;32m53\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "if jerarquico_agua:\n",
    "    water_dict = {21:1, 22:2}\n",
    "    \n",
    "    dict_clases = {1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10,\n",
    "               11:11, 12:12, 13:13, 14:14, 15:15, 16:16, 17:17, 18:18, 19:19, 20:20,\n",
    "               21:21, 22:21, 23:22, 24:23, 25:24, 26:25, 27:26, 28:27, 29:28}\n",
    "\n",
    "\n",
    "    y_train_coded = tf.one_hot(y_train_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "    y_val_coded = tf.one_hot(y_val_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "    y_test_coded = tf.one_hot(y_test_general-1, depth=nclasses, on_value=1, off_value=0)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_train), tf.convert_to_tensor(y_train_coded)))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_val), tf.convert_to_tensor(y_val_coded)))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(x_test), tf.convert_to_tensor(y_test_coded)))\n",
    "\n",
    "    train_batches = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "    val_batches = val_dataset.batch(batch_size)\n",
    "    test_batches = test_dataset.batch(batch_size)\n",
    "\n",
    "    print(model_avgg.summary())\n",
    "\n",
    "    test_probs = model_avgg.predict(x_test)\n",
    "    test_preds = np.argmax(test_probs, axis=-1) + 1\n",
    "\n",
    "    print(np.max(test_preds))\n",
    "    key_list=list(dict_clases.keys())\n",
    "    val_list=list(dict_clases.values())\n",
    "\n",
    "    for i in range(test_preds.shape[0]):\n",
    "        key_list=list(dict_clases.keys())\n",
    "        val_list=list(dict_clases.values())\n",
    "        ind=val_list.index(test_preds[i])\n",
    "        test_preds[i] = key_list[ind]\n",
    "\n",
    "        if test_preds[i] == 21:\n",
    "            pca_test_i = pca.transform(x_test[i].reshape(1,-1))\n",
    "            new_pred = svm.predict(pca_test_i)\n",
    "            print(new_pred)\n",
    "            key_list=list(water_dict.keys())\n",
    "            print(key_list)\n",
    "            val_list=list(water_dict.values())\n",
    "            print(val_list)\n",
    "            ind=val_list.index(new_pred)\n",
    "            print(ind)\n",
    "            test_preds[i] = key_list[ind]\n",
    "\n",
    "\n",
    "    \n",
    "    print(calcularAccuracy(y_test, test_preds))\n",
    "\n",
    "else:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d8cd3c601c15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_matrix\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mtest_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m axlabs = [\n\u001b[0;32m      5\u001b[0m   \u001b[1;34m\"1_BarrenLands\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_preds' is not defined"
     ]
    }
   ],
   "source": [
    "  test_matrix = confusion_matrix(y_true=y_test, y_pred=test_preds).astype(np.float)\n",
    "  test_matrix /= test_matrix.astype(np.float).sum(axis=1)\n",
    "\n",
    "  axlabs = [\n",
    "    \"1_BarrenLands\",\n",
    "    \"2_MossAndLichen\",\n",
    "    \"3_MossAndLichen\",\n",
    "    \"4_SrublandOpen\",\n",
    "    \"5_SrublandClose\",\n",
    "    \"6_ForestsOpDeBr\",\n",
    "    \"7_ForestClDeBr\",\n",
    "    \"8_ForestsDeDeBr\",\n",
    "    \"9_ForestsOpDeNe\",\n",
    "    \"10_ForestsClDeNe\",\n",
    "    \"11_ForestsDeDeNe\",\n",
    "    \"12_ForestsOpEvBr\",\n",
    "    \"13_ForestsClEvBr\",\n",
    "    \"14_ForestsDeEvBr\",\n",
    "    \"15_ForestsOpEvNe\",\n",
    "    \"16_ForestsClEvNe\",\n",
    "    \"17_ForestsDeEvNe\",\n",
    "    \"18_WetlandMangro\",\n",
    "    \"19_WetlandSwamps\",\n",
    "    \"20_WetlandMarshl\",\n",
    "    \"21_WaterBodyMari\",\n",
    "    \"22_WaterBodyCont\",\n",
    "    \"23_PermanentSnow\",\n",
    "    \"24_CropSeasWater\",\n",
    "    \"25_CropCerealrri\",\n",
    "    \"26_CropCereaRain\",\n",
    "    \"27_CropBroadlrri\",\n",
    "    \"28_CropBroadRain\",\n",
    "    \"29_UrbanBlUpArea\",\n",
    "\n",
    "  ]\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10, 20))         # Sample figsize in inches\n",
    "  sns.heatmap(test_matrix, annot=True, cmap='Blues', cbar=False, linewidths=1, square=True, xticklabels=axlabs, yticklabels=axlabs)\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-805ed5f93b42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'id.jpg'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnames_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'david/predicciones/pred'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msubida\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_preds' is not defined"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame({'id.jpg': names_test, 'label':test_preds})\n",
    "output.to_csv('david/predicciones/pred' + subida + '.csv', header=True, index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b45b4423d69a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdict_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'id.jpg'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnames_test\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdict_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prob_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'david/predicciones/pred_probs'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msubida\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_probs' is not defined"
     ]
    }
   ],
   "source": [
    "dict_probs = {'id.jpg': names_test}\n",
    "for i in range(test_probs.shape[1]):\n",
    "    dict_probs['prob_' + str(i)] = test_probs[:,i]\n",
    "output = pd.DataFrame(dict_probs)\n",
    "output.to_csv('david/predicciones/pred_probs' + subida + '.csv', header=True, index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = False\n",
    "ensemble = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2cc014fa6324>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mpca_test_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mnew_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_test_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mkey_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwater_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pca' is not defined"
     ]
    }
   ],
   "source": [
    "if ensemble:\n",
    "    \n",
    "    subida_ensemble = '6'\n",
    "    ensemble_method = \"weighted_average\"\n",
    "    \n",
    "    models = []\n",
    "    models.append(pd.read_csv('david/predicciones/pred_probs17.csv').iloc[:, 1:].to_numpy())\n",
    "    models.append(pd.read_csv('david/predicciones/pred_probs16.csv').iloc[:, 1:].to_numpy())\n",
    "    models.append(pd.read_csv('david/predicciones/pred_probs15.csv').iloc[:, 1:].to_numpy())\n",
    "    models.append(pd.read_csv('david/predicciones/pred_probs14.csv').iloc[:, 1:].to_numpy())\n",
    "    models.append(pd.read_csv('david/predicciones/pred_probs13.csv').iloc[:, 1:].to_numpy())\n",
    "    models.append(pd.read_csv('david/predicciones/pred_probs18.csv').iloc[:, 1:].to_numpy())\n",
    "    other_model = pd.read_csv('david/predicciones/pred10.csv').iloc[:,1:]\n",
    "    #models.append(pd.read_csv('david/predicciones/pred_probs19.csv').iloc[:, 1:].to_numpy())\n",
    "    #models.append(pd.read_csv('david/predicciones/pred_probs20.csv').iloc[:, 1:].to_numpy())\n",
    "    #models.append(pd.read_csv('david/predicciones/pred_probs21.csv').iloc[:, 1:].to_numpy())\n",
    "    #models.append(pd.read_csv('david/predicciones/pred_probs22.csv').iloc[:, 1:].to_numpy())\n",
    "    #models.append(pd.read_csv('david/predicciones/pred_probs23.csv').iloc[:, 1:].to_numpy())\n",
    "\n",
    "\n",
    "    \n",
    "    models = np.array(models)\n",
    "    \n",
    "    if ensemble_method == \"average\":\n",
    "        test_preds = np.argmax(np.sum(models, axis=0), axis = -1) + 1\n",
    "    \n",
    "    if ensemble_method == \"weighted_average\":\n",
    "        for i in range(models.shape[0]):\n",
    "            models[i] = models[i]*calcularAccuracy(y_test, np.argmax(models[i], axis = -1) + 1)\n",
    "        test_preds = np.argmax(np.sum(models, axis=0) / float(models.shape[0]), axis = -1) + 1\n",
    "        \n",
    "    if ensemble_method == \"max\":\n",
    "        test_preds = []\n",
    "        for i in range(models.shape[1]):\n",
    "            dict_max = {}\n",
    "            dict_argmax = {}\n",
    "            for j in range(models.shape[0]):\n",
    "                dict_max[j] = np.max(models[j][i])\n",
    "                dict_argmax[j] = np.argmax(models[j][i])\n",
    "            test_preds.append(dict_argmax[np.argmax(list(dict_max.values()))] + 1)\n",
    "        test_preds = np.array(test_preds)\n",
    "    \n",
    "    \n",
    "    # Correccion water\n",
    "    for i in range(test_preds.shape[0]):\n",
    "        key_list=list(dict_clases.keys())\n",
    "        val_list=list(dict_clases.values())\n",
    "        ind=val_list.index(test_preds[i])\n",
    "        test_preds[i] = key_list[ind]\n",
    "\n",
    "        if test_preds[i] == 21:\n",
    "            pca_test_i = pca.transform(x_test[i].reshape(1,-1))\n",
    "            new_pred = svm.predict(pca_test_i)\n",
    "            key_list=list(water_dict.keys())\n",
    "            val_list=list(water_dict.values())\n",
    "            ind=val_list.index(new_pred)\n",
    "            test_preds[i] = key_list[ind]\n",
    "    \n",
    "    test_matrix = confusion_matrix(y_true=y_test, y_pred=test_preds).astype(np.float)\n",
    "    test_matrix /= test_matrix.astype(np.float).sum(axis=1)\n",
    "    \n",
    "    print(calcularAccuracy(y_test, test_preds))\n",
    "    fig, ax = plt.subplots(figsize=(10, 20))         # Sample figsize in inches\n",
    "    sns.heatmap(test_matrix, annot=True, cmap='Blues', cbar=False, linewidths=1, square=True, xticklabels=axlabs, yticklabels=axlabs)\n",
    "    plt.show()\n",
    "    \n",
    "    output = pd.DataFrame({'id.jpg': names_test, 'label':test_preds})\n",
    "    output.to_csv('david/predicciones/pred_ensemble' + subida + '.csv', header=True, index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9592088998763906\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.read_csv('david/predicciones/pred_ensemble22.csv')\n",
    "print(calcularAccuracy(y_test, new_data.iloc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MINDAT_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
