{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_text\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import utils\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "print(F\"GPUS: {tf.config.list_physical_devices()}\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"resnet152_baseline\")\n",
    "parser.add_argument(\"--DA_name\", type=str, default=\"DA1\")\n",
    "parser.add_argument(\"--load_model\", type=bool, default=False)\n",
    "parser.add_argument(\"--ft_mode\", type=int, default=0)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "\n",
    "ARGS = parser.parse_args()\n",
    "\n",
    "model_name = ARGS.model_name\n",
    "base_model_name = model_name.split(\"_\")[0]\n",
    "top_model_name = model_name.split(\"_\")[1]\n",
    "DA_name = ARGS.DA_name\n",
    "load = ARGS.load_model\n",
    "\n",
    "MAIN_PATH = \"/mnt/homeGPU/fcastro/lulc/\"\n",
    "\n",
    "#data_dir = \"../reduced_data/\"\n",
    "data_dir = MAIN_PATH + \"data/\"\n",
    "batch_size = ARGS.batch_size\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "early_stop = True\n",
    "patience = 10\n",
    "epochs = 200\n",
    "learning_rate = ARGS.lr\n",
    "ft_mode = ARGS.ft_mode\n",
    "\n",
    "if load:\n",
    "    results_name = f\"{model_name}_loadft{ft_mode}_{DA_name}\"\n",
    "else:\n",
    "    results_name = f\"{model_name}_ft{ft_mode}_{DA_name}\"\n",
    "\n",
    "weights_dir = MAIN_PATH + \"weights/\"\n",
    "load_weights_file = f\"{base_model_name}_{top_model_name}_ft0_{DA_name}.h5\"\n",
    "save_weights_file = f\"{results_name}\"\n",
    "load_weights_path = weights_dir + load_weights_file\n",
    "save_weights_path = weights_dir + save_weights_file\n",
    "\n",
    "submission_dir = MAIN_PATH + \"submissions/\"\n",
    "submission_file = f\"{results_name}.csv\"\n",
    "submission_path = submission_dir + submission_file\n",
    "\n",
    "submission_history_path = MAIN_PATH + f\"submission_history.csv\"\n",
    "\n",
    "train_data, train_labels, test_data, test_labels, test_names = utils.load_data(data_dir, norm=False)\n",
    "\n",
    "prep_fn = models.get_prep_fn(base_model_name)\n",
    "if prep_fn is not None:\n",
    "    train_data = prep_fn(train_data)\n",
    "\n",
    "train_labels = tf.one_hot(train_labels, 29).numpy()\n",
    "test_labels = tf.one_hot(test_labels, 29).numpy()\n",
    "\n",
    "idx = np.arange(len(train_data))\n",
    "train_idx, val_idx = train_test_split(idx,test_size=0.1, random_state=0)\n",
    "val_data = train_data[val_idx]\n",
    "val_labels = train_labels[val_idx]\n",
    "train_data = train_data[train_idx]\n",
    "train_labels = train_labels[train_idx]\n",
    "\n",
    "DA_fn, DA_test_fn = models.get_DA_fn(DA_name)\n",
    "\n",
    "if DA_fn is not None:\n",
    "    print(\"using DA in training\")\n",
    "if DA_test_fn is not None:\n",
    "    print(\"using DA in val and test\")\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=DA_fn)\n",
    "train_generator.fit(train_data)\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=DA_test_fn)\n",
    "val_generator.fit(val_data)\n",
    "\n",
    "model = models.build_model(base_model_name, top_model_name, ft_mode)\n",
    "\n",
    "if (load):\n",
    "    print(\"Loading model\")\n",
    "    model.load_weights(load_weights_path)\n",
    "\n",
    "\"\"\"\n",
    "if ft_mode==1:\n",
    "    base_model = model.layers[1]\n",
    "    base_model.trainable = True\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "num_layers = len(base_model.layers)\n",
    "for layer in base_model.layers[num_layers-ft_layers:]:\n",
    "        layer.trainable = True\n",
    "print(num_layers)\n",
    "print(len(base_model.trainable_variables))\n",
    "print(len(model.trainable_variables))\n",
    "\"\"\"\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=True)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "metrics = [tf.keras.metrics.categorical_accuracy]\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=patience, verbose=0, mode='auto', restore_best_weights=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=0, mode='auto', restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(save_weights_path, save_best_only=True, save_weights_only=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.000001)\n",
    "\n",
    "callbacks = [reduce_lr]\n",
    "#callbacks = [model_checkpoint, reduce_lr]\n",
    "if early_stop:\n",
    "    callbacks = callbacks + [early_stopping]\n",
    "\n",
    "train_it = train_generator.flow(train_data, train_labels, batch_size)\n",
    "val_it = val_generator.flow(val_data, val_labels, batch_size)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "hist = model.fit(train_it, validation_data=val_it, epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "model.save_weights(f\"{save_weights_path}.h5\", save_format='h5')\n",
    "#tf.keras.models.save_model(model, f\"{save_weights_path}.h5\", save_format='h5')\n",
    "\n",
    "preds = model.predict(train_data, batch_size)\n",
    "train_acc = accuracy_score(np.argmax(train_labels, axis=-1), np.argmax(preds, axis=-1))\n",
    "print(f\"Train Acc.: {train_acc}\")\n",
    "\n",
    "if prep_fn is not None:\n",
    "    test_data = prep_fn(test_data)\n",
    "\n",
    "if DA_test_fn is not None:\n",
    "    test_data = DA_test_fn(test_data).numpy()\n",
    "\n",
    "preds = model.predict(test_data, batch_size)\n",
    "test_acc = accuracy_score(np.argmax(test_labels, axis=-1), np.argmax(preds, axis=-1))\n",
    "print(f\"Test Acc.: {test_acc}\")\n",
    "preds_argmax = np.argmax(preds, axis=-1)\n",
    "\n",
    "d = {'id.jpg': test_names, 'label': preds_argmax}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(f\"Writing {submission_path}\")\n",
    "df.to_csv(submission_path, index=False)\n",
    "print(\"Done\")\n",
    "\n",
    "d = {'submission_name': [results_name], 'train_acc': [train_acc], 'test_acc': [test_acc]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(f\"Updating {submission_history_path}\")\n",
    "df.to_csv(submission_history_path, index=False, mode='a', header=False)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
